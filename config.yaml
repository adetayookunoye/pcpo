seed: 42
seeds: [0, 1, 2, 3, 4]

data:
  root: ./data_cache
  dataset: pdebench_ns2d
  pdebench_id: ns_incom
  processed_pattern: ns_incom*_ds8_stride20_off4.npz
  grid_size: 64
  t_steps: 5
  train_split: 0.8
  synthetic_if_missing: true
  synthetic_samples: 64
  normalize: true
  normalize_target_separately: true
  standardize_energy: false
  augment_ic_perturb: 0.01

model:
  type: divfree_fno              # per-run override via --model
  modes: 24                      # + capacity helps small-scale fidelity
  width: 72
  latent_dim: 64                 # for cVAE; ignored by deterministic
  cvae_beta: 0.3                 # lower β → less collapse
  physics_weight: 0.0            # only PINO uses physics residual
  nu: 1.0e-3
  deep_hidden: [96, 96, 96]
  amplitude_calibration: true    # learn a single gain on (u,v) head
  amplitude_init: 1.0
  amplitude_reg: 1.0e-6
  spectral_bins: 16              # for spectral loss/metrics (log-binned k)

pino:
  physics_weight: 1.0e-3         # only applied when model == pino
  nu: 1.0e-3

train:
  epochs: 350
  batch_size: 16                 # ↑ if memory allows; else keep 12
  lr: 1.0e-3
  weight_decay: 1.0e-5
  log_every: 20
  save_every: 1
  # IMPORTANT: these are OPTIMIZER STEPS, not epochs
  lr_schedule:
    type: cosine
    total_steps: auto            # trainer computes steps_per_epoch*epochs
    warmup_steps: 200           # ~1–3% of total steps
    end_value: 1.0e-5            # nonzero tail helps stability
  kl_warmup_epochs: 150          # longer warm-up combats collapse
  kl_target_beta: 0.3
  kl_free_bits: 0.75
  cvae_input_noise: 0.03
  preview_rows: 20
  grad_clip_norm: 1.0
  ema_decay: 0.9995              # slightly stronger EMA
  checkpoint_metric: val_l2      # save best
  early_stop_patience: 30        # optional

loss:
  l2: 1.0
  div: 0.0                       # keep 0 for divfree/cVAE; set 1.0 for plain FNO runs
  energy: 0.02                   # tiny term to fix amplitude (with amplitude_calibration)
  vorticity_l2: 0.1
  h1: 0.0
  spectral_mse: 0.05             # NEW: MSE on log-spectrum (uses spectral_bins)

# Trainer should internally route:
# - add(div=1.0) only if model in {fno, pino, bayes_deeponet}
# - add(physics_weight) only if model == pino

metrics:
  energy_ref_from_input: false
  report_mean_speed: true
  spectra_ratio: true
  rollout:
    steps: 10                    # track L2(t), div(t), energy drift(t)

outputs:
  results_dir: ./results
  figures_dir: ./results/figures
  checkpoints_dir: ./results/{model}/checkpoints

compare:
  seeds: [0, 1, 2, 3, 4]
  n_samples: 64                  # better CRPS/coverage stability
  models: ["fno","pino","bayes_deeponet","divfree_fno","cvae_fno"]
  rollout_steps: 10
