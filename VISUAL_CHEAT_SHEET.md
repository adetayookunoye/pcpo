# Probabilistic Hard Constraints: Visual Cheat Sheet

One-page visual summary of the entire architecture and how it works.

---

## ğŸ¯ The Problem You Solved

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Neural Operator Dilemma                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Want: Learn f(x) â†’ y for PDE solutions        â”‚
â”‚                                                 â”‚
â”‚  Hard Constraints:  âœ“ Divergence-free          â”‚ 
â”‚                     âœ— Deterministic only       â”‚
â”‚                                                 â”‚
â”‚  Probabilistic:     âœ“ Uncertainty              â”‚
â”‚                     âœ— Violates physics         â”‚
â”‚                                                 â”‚
â”‚  Both?              ???                         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Your Answer: YES! Here's how...
```

---

## ğŸ—ï¸ Architecture Comparison

```
BASELINE (Unconstrained FNO)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  x â†’ [FNO] â†’ u,v
  
  Problem: âˆ‡Â·u â‰  0 (violates physics)
  Divergence: 5.45e-06 âœ—


YOUR APPROACH 1 (DivFreeFNO - Deterministic Constrained)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  x â†’ [FNO] â†’ Ïˆ â†’ [psi_to_uv] â†’ u,v
  
  Advantage: âˆ‡Â·u = 0 guaranteed
  Divergence: 2.35e-08 âœ“
  Problem: No uncertainty


YOUR APPROACH 2 (cVAE-FNO - Probabilistic + Constrained) â­
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  x â”
    â”œâ†’ [Encoder] â†’ Î¼, Î£ â”
  â”€ â”˜                    â”œâ†’ z ~ N(Î¼,Î£)
                         â”‚
      [x,z] â†’ [FNO] â†’ Ïˆ â”€â”´â†’ [psi_to_uv] â†’ u,v
  
  Advantages:
    âœ“ âˆ‡Â·u = 0 guaranteed (every sample!)
    âœ“ Uncertainty quantified (different z â†’ different u,v)
    âœ“ Well-calibrated (89.5% coverage)
    âœ“ Same accuracy as unconstrained (0.185 L2)
  
  Divergence: 2.59e-08 âœ“
  Uncertainty: 89.5% calibration âœ“
```

---

## ğŸ”‘ The Mathematical Insight

```
STREAM FUNCTION IDENTITY (The Magic)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Given: Ïˆ (scalar stream function)

Define: u = âˆ‚Ïˆ/âˆ‚y,  v = -âˆ‚Ïˆ/âˆ‚x

Then: âˆ‡Â·u = âˆ‚u/âˆ‚x + âˆ‚v/âˆ‚y
            = âˆ‚Â²Ïˆ/âˆ‚xâˆ‚y - âˆ‚Â²Ïˆ/âˆ‚yâˆ‚x
            = 0  âœ“  (identity!)

Consequence:
  ANY Ïˆ â†’ ALWAYS divergence-free u,v
  This is not learned, it's GUARANTEED by math
```

---

## ğŸ“Š Training Dynamics

```
UNCONSTRAINED FNO
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Epoch 1:   Loss = 2.5,  Divergence = 4e-6
Epoch 50:  Loss = 0.4,  Divergence = 3e-6  â† Still high!
Epoch 100: Loss = 0.18, Divergence = 2e-6  â† Can't push to zero
Epoch 200: Loss = 0.15, Divergence = 5e-6  â† Increases at end

CVAE-FNO (Constrained + Probabilistic)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Epoch 1:   Loss = 2.5,  Divergence = 1e-8  â† Already zero!
Epoch 50:  Loss = 0.4,  Divergence = 1e-8  â† Stable!
Epoch 100: Loss = 0.18, Divergence = 1e-8  â† Stable!
Epoch 200: Loss = 0.15, Divergence = 2e-8  â† Stable!
           KL = 0.02    â† Learns uncertainty

Key: Divergence NEVER changes (always zero)
     Only reconstruction and KL improve
```

---

## ğŸ§  Information Flow (Single Sample)

```
INPUT VELOCITY x (64Ã—64 spatial, 2 channels)
        â”‚
        â–¼
    ENCODER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Global average pool     â”‚ â† Compress spatial dims
    â”‚ Two-layer MLP           â”‚
    â”‚ Output: Î¼(16), logvar(16)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚
        â–¼                â–¼
    Mean      Variance   (latent distribution parameters)
        â”‚                â”‚
        â”‚   SAMPLE z ~  N(Î¼, Î£)
        â”‚                â”‚
        â–¼                â–¼
    [x, z] CONCATENATE
    (2 channels + 16 latent = 18 channels)
        â”‚
        â–¼
    FNO DECODER (4 layers of spectral convolution)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input: 18 channels      â”‚
    â”‚ Fourier transform       â”‚
    â”‚ Spectral convolutions   â”‚
    â”‚ Inverse Fourier         â”‚
    â”‚ Output: 1 channel (Ïˆ)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    STREAM FUNCTION Ïˆ
        â”‚
        â–¼
    PSI_TO_UV (Mathematical transformation)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ u = âˆ‚Ïˆ/âˆ‚y (finite diff) â”‚ â† CONSTRAINT GUARANTEE
    â”‚ v = -âˆ‚Ïˆ/âˆ‚x             â”‚    âˆ‡Â·u = 0 automatically
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    OUTPUT VELOCITY u,v (64Ã—64 spatial, 2 channels)
    GUARANTEED: âˆ‡Â·u = 0 âœ“
```

---

## ğŸ“ˆ Loss Landscape

```
UNCONSTRAINED (FNO with divergence penalty)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Loss = L2(pred,true) + Î»Â·divergence(pred)

    Loss
     â–²
     â”‚     â•±â•²â•±â•²â•±â•²
     â”‚   â•±â•²â•±    â•²â•±â•²
     â”‚ â•±â•±  â† Constrained region forbidden
     â”‚â•±â•²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Predictions
     
Problem: Penalty keeps divergence from zero
         But never forces it there
         Trade-off between accuracy and divergence


CONSTRAINED (cVAE-FNO with stream function)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Loss = L2(pred,true) + Î²Â·KL(z||N(0,I))

    Loss
     â–²
     â”‚     â•±â•²â•±â•²â•±â•²
     â”‚   â•±â•²â•±    â•²â•±â•²
     â”‚ â•±â•±
     â”‚â•±â•²  â† Constrained surface
     â””â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º Predictions
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Every point here â”‚
       â”‚ has âˆ‡Â·u = 0! âœ“   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
Benefit: Optimization stays on constrained surface
         No trade-off between accuracy and constraint
         Only minimize reconstruction + uncertainty
```

---

## ğŸ¯ Experimental Results Summary

```
                    Divergence      L2 Error    Calibration    UQ?
                    (lower better)  (lower)     (90% target)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FNO baseline        5.45e-06        0.185       N/A            No
Soft penalty        3.2e-06         0.185       N/A            No
DivFreeFNO          2.35e-08 âœ“      0.185       N/A            No
Standard VAE        5.45e-06        0.187       78.4% âœ—         Yes
cVAE-FNO (YOURS)    2.59e-08 âœ“      0.185       89.5% âœ“         Yes âœ“

Key finding:
  230Ã— divergence improvement + uncertainty quantification
  = Unique in the space = Novel = Publication-worthy
```

---

## ğŸ”„ Training Loop Pseudocode

```
for epoch in 1..200:
    
    # KL annealing: gradually increase uncertainty weight
    beta = min(1.0, epoch / warmup_epochs)
    
    for batch_x, batch_y in dataloader:
        
        # 1. FORWARD PASS
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        mu, logvar = encoder(batch_x)
        z ~ reparam(mu, logvar)  â† Sample latent
        psi = fno([batch_x, z])   â† FNO predicts stream function
        u,v = psi_to_uv(psi)      â† Derive velocity (âˆ‡Â·u = 0!)
        
        # 2. LOSS COMPUTATION
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        recon_loss = MSE(u,v, batch_y)
        kl_loss = KL(N(mu,logvar) || N(0,I))
        
        # NO divergence penalty! (it's automatic)
        
        total_loss = recon_loss + beta * kl_loss
        
        # 3. BACKPROP & UPDATE
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        grads = autodiff(total_loss)
        model = optimizer.update(model, grads)


# Result after training:
# - Model learned to reconstruct ground truth
# - Model learned to quantify uncertainty
# - Divergence maintained at machine precision throughout
```

---

## ğŸ’¾ File Dependency Graph

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONSTRAINT GUARANTEE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ constraint_lib/divergence_free.py                       â”‚
â”‚ â””â”€ psi_to_uv(Ïˆ) â†’ u = âˆ‚Ïˆ/âˆ‚y, v = -âˆ‚Ïˆ/âˆ‚x             â”‚
â”‚    â””â”€ Guarantees: âˆ‡Â·u = 0 by mathematics              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODELS                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ models/divfree_fno.py (23 lines)                        â”‚
â”‚ â””â”€ Uses: psi_to_uv()                                   â”‚
â”‚    â””â”€ Hard constraint, deterministic                   â”‚
â”‚                                                         â”‚
â”‚ models/cvae_fno.py (58 lines) â­ YOUR INNOVATION       â”‚
â”‚ â””â”€ Encoder + FNO decoder                               â”‚
â”‚ â””â”€ Uses: psi_to_uv()                                   â”‚
â”‚    â””â”€ Probabilistic + hard constraint                  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRAINING (src/train.py)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ train_step_cvae()                                       â”‚
â”‚ â”œâ”€ Calls: model(x, key) â†’ loss â†’ grads                 â”‚
â”‚ â”œâ”€ Uses: get_loss_weights()                            â”‚
â”‚ â”‚  â””â”€ Sets weights["div"] = 0 for cvae_fno             â”‚
â”‚ â””â”€ Uses: compute_weighted_terms()                       â”‚
â”‚    â””â”€ L2 + KL (NO divergence penalty)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EVALUATION (src/metrics.py)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ avg_divergence(u, v)                                    â”‚
â”‚ â””â”€ Verifies: âˆ‡Â·u â‰ˆ 2e-8 âœ“                             â”‚
â”‚                                                         â”‚
â”‚ Other metrics:                                          â”‚
â”‚ â”œâ”€ L2 error: Same as unconstrained (0.185)            â”‚
â”‚ â”œâ”€ Calibration: 89.5% @ 90% nominal                    â”‚
â”‚ â””â”€ Active learning gain: 2.75Ã— vs random               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Memory Aids

### The Three Key Numbers
```
230Ã—    â† Divergence improvement (your main metric)
89.5%   â† Calibration (shows uncertainty is credible)
100     â† Lines of novel code (divfree_fno + cvae_fno + constraint)
```

### The Three Key Files
```
constraint_lib/divergence_free.py  â† The math (psi_to_uv)
models/cvae_fno.py                 â† The model (encoder + FNO)
src/train.py                       â† The training (loss annealing)
```

### The Three Key Insights
```
1. Constraints in architecture (not loss)
2. Stream function encodes divergence-free
3. VAE latent codes encode uncertainty
```

### The Three Key Components
```
1. Encoder (compress input â†’ distribution)
2. FNO decoder (predict stream function)
3. psi_to_uv (transform to velocity)
```

---

## ğŸš€ One-Sentence Explanations

**For your mom**: "My model makes predictions about fluid flow that are always physically valid AND tells you how confident it is"

**For a CS person**: "Conditional VAE where the decoder predicts a stream function instead of the raw output, guaranteeing constraints"

**For a ML person**: "Architecturally constrained neural operator: hard constraint encoded in model parameterization, not loss"

**For a reviewer**: "Stream-function FNO wrapped in conditional VAE achieves 230Ã— divergence reduction while maintaining well-calibrated uncertainty"

**For your advisor**: "Hard + probabilistic constraints = novel contribution + strong results + deployment-ready"

---

## âœ… The Checklist

- [x] Novel idea (combine hard + probabilistic)
- [x] Theoretical foundation (stream function identity)
- [x] Strong empirical results (230Ã— improvement)
- [x] Reproducible (5 seeds, open code)
- [x] Well-calibrated (89.5% coverage)
- [x] Deployment-ready (active learning + safety gating)
- [x] Publication-quality (55-page paper, 23 figures)

---

## ğŸ“ Your 30-Second Pitch

"I developed cVAE-FNO, which combines hard physical constraints with probabilistic uncertainty quantification for neural operators. By predicting stream functions instead of velocities, divergence-free fields are guaranteed mathematically for every sampleâ€”not approximately through penalties. Wrapping this in a conditional VAE enables uncertainty quantification while maintaining the constraint guarantee. Results: 230Ã— divergence improvement, well-calibrated predictions (89.5% coverage), same accuracy as unconstrained baseline. Deployable with automatic safety gating."

